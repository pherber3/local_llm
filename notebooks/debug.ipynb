{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files in vectorstore:\n",
      "ðŸ“„ analysis.py: 68 chunks\n",
      "ðŸ“„ base_model.py: 4 chunks\n",
      "ðŸ“„ benefit_map.py: 1 chunks\n",
      "ðŸ“„ config.py: 7 chunks\n",
      "ðŸ“„ constants_calculator.py: 40 chunks\n",
      "ðŸ“„ custom_warnings.py: 1 chunks\n",
      "ðŸ“„ data_ingestion.py: 19 chunks\n",
      "ðŸ“„ exceptions.py: 2 chunks\n",
      "ðŸ“„ external_data_processor.py: 49 chunks\n",
      "ðŸ“„ flask_responses.py: 9 chunks\n",
      "ðŸ“„ global_weights.py: 7 chunks\n",
      "ðŸ“„ heuristic_model_v1.py: 16 chunks\n",
      "ðŸ“„ heuristic_model_v2.py: 20 chunks\n",
      "ðŸ“„ invocations_schema.py: 15 chunks\n",
      "ðŸ“„ neural_network_model.py: 25 chunks\n",
      "ðŸ“„ node.py: 15 chunks\n",
      "ðŸ“„ optimization.py: 55 chunks\n",
      "ðŸ“„ payload_parser.py: 56 chunks\n",
      "ðŸ“„ plan_fit_graph.py: 55 chunks\n",
      "ðŸ“„ preprocessor.py: 88 chunks\n",
      "ðŸ“„ sample_model.py: 2 chunks\n",
      "ðŸ“„ trainable_heuristic_model.py: 31 chunks\n",
      "ðŸ“„ utilities.py: 12 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize document processor\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "loader = DirectoryLoader(\n",
    "    \"/Users/pherbert/Documents/GoHealth Projects/model-plan-recommendation/modelplanrecommendation\",  # Or your specific path\n",
    "    glob=[\"**/*.py\", \"**/*.txt\", \"**/*.md\"],\n",
    "    loader_cls=TextLoader,\n",
    ")\n",
    "docs = loader.load()\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create vectorstore\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# Create retrieval chain\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "llm = ChatOllama(model=\"deepseek-r1:14b\")\n",
    "\n",
    "# Build basic QA chain\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer questions based on the provided context.\n",
    "\n",
    "Context:\n",
    "{local_docs}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | RunnablePassthrough.assign(\n",
    "        local_docs=lambda x: retriever.invoke(x[\"question\"])\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Debug what's in the vectorstore\n",
    "result = vectorstore.get()\n",
    "file_counts = {}\n",
    "for metadata in result['metadatas']:\n",
    "    source = metadata.get('source', 'Unknown')\n",
    "    filename = os.path.basename(source)\n",
    "    file_counts[filename] = file_counts.get(filename, 0) + 1\n",
    "\n",
    "print(\"\\nFiles in vectorstore:\")\n",
    "for filename, count in sorted(file_counts.items()):\n",
    "    print(f\"ðŸ“„ {filename}: {count} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved documents for query: 'summarize trainable_heuristic_model.py'\n",
      "\n",
      "1. config.py\n",
      "Content preview: # all available models to be selected from\n",
      "model_dict = {\n",
      "    \"heuristic_v1\": HeuristicModelOne(),\n",
      "    \"heuristic_v2\": HeuristicModelTwo(GLOBAL_WEIGHTS_HEURISTIC_V2),\n",
      "    \"heuristic_v2.1\": HeuristicMo...\n",
      "\n",
      "2. heuristic_model_v2.py\n",
      "Content preview: class HeuristicModelTwo(BaseModel):\n",
      "    def __init__(\n",
      "        self,\n",
      "        global_weights: dict,\n",
      "        model_version: str = \"1.0\",\n",
      "        logging_level: int = logging.DEBUG,\n",
      "        is_simulation:...\n",
      "\n",
      "3. heuristic_model_v2.py\n",
      "Content preview: @property\n",
      "    def model_name(self) -> str:\n",
      "        model_name = \"heuristic_python\"\n",
      "        return model_name\n",
      "\n",
      "    @property\n",
      "    def model_version(self) -> str:\n",
      "        return self._model_version\n",
      "\n",
      "    ...\n",
      "\n",
      "4. config.py\n",
      "Content preview: ),\n",
      "    \"heuristic_v2.6\": HeuristicModelTwo(\n",
      "        global_weights=GLOBAL_WEIGHTS_HEURISTIC_V2_6, model_version=\"1.6\"\n",
      "    ),\n",
      "    \"heuristic_v2.7\": HeuristicModelTwo(\n",
      "        global_weights=GLOBAL_WEIG...\n",
      "\n",
      "5. neural_network_model.py\n",
      "Content preview: class NeuralNetworkModel(HeuristicModelTwo):\n",
      "    model_name = \"neural_network_v1\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        model_version: str = \"2.2\",\n",
      "        graph_feature_version: str = \"2.2\",\n",
      "     ...\n",
      "\n",
      "Full response from model:\n",
      "<think>\n",
      "Okay, so I need to figure out how to summarize the content of a file called trainable_heuristic_model.py. But wait, looking at the context provided, it seems that there isn't any direct mention or content from this specific file. The documents included are about model configurations, HeuristicModelTwo, and NeuralNetworkModel, but none of them seem to relate directly to trainable_heuristic_model.py.\n",
      "\n",
      "Hmm, maybe I should check again. The first document is config.py with a model_dict containing various heuristic models like v2 through v2.8 and some neural network models. Then there's the HeuristicModelTwo class in heuristic_model_v2.py which has an __init__ method and properties for model_version, graph generation, external data loading, etc.\n",
      "\n",
      "Another part of the context shows a property decorator for model_name and model_version, with some versioning logic where the model_version is converted to a float and then back to a string. There's also a predict method decorated with New Relic tracing.\n",
      "\n",
      "Looking at another document from config.py, it extends the model_dict up to v2.8 and includes neural network models. The last document is neural_network_model.py which defines NeuralNetworkModel inheriting from HeuristicModelTwo. It has an __init__ method that sets various versions and calls super().__init__() with global_weights based on graph_feature_version.\n",
      "\n",
      "But still, there's no content about trainable_heuristic_model.py. So I can't extract specific information from it because it's not present in the provided context. Without knowing what this file contains, like any classes, methods, or functionalities, I can't provide a meaningful summary. It's possible that this file might have some other heuristic model implementations or additional training logic, but without seeing its content, I'm stuck.\n",
      "\n",
      "I should probably inform the user that based on the provided context, there's no information about trainable_heuristic_model.py available for summarization. Alternatively, if they can provide more details or clarify which parts of the existing documents relate to it, I could help further.\n",
      "</think>\n",
      "\n",
      "Based on the provided context, there is no content related to the file \"trainable_heuristic_model.py\" that can be summarized. The documents mention model configurations and specific classes like HeuristicModelTwo and NeuralNetworkModel, but these do not directly pertain to the file in question.\n",
      "\n",
      "If you have access to the contents of \"trainable_heuristic_model.py,\" feel free to share its details so I can provide a meaningful summary!\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval for a specific file\n",
    "question = \"summarize trainable_heuristic_model.py\"\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "print(f\"\\nRetrieved documents for query: '{question}'\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    source = doc.metadata.get('source', 'Unknown')\n",
    "    filename = os.path.basename(source)\n",
    "    print(f\"\\n{i}. {filename}\")\n",
    "    print(f\"Content preview: {doc.page_content[:200]}...\")\n",
    "\n",
    "# Try the actual query\n",
    "response = chain.invoke({\"question\": question})\n",
    "print(\"\\nFull response from model:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 31 chunks from trainable_heuristic_model.py:\n",
      "\n",
      "Chunk 1:\n",
      "import logging\n",
      "\n",
      "import newrelic.agent\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "from modelplanrecommendation.businesslogic.external_data_processor import (\n",
      "    append_external_data,\n",
      "    get_ext_data_scores,\n",
      ")\n",
      "from modelplanrecommendati ...\n",
      "\n",
      "Chunk 2:\n",
      "MODEL_VERSIONS_TO_APPEND_EXT_DATA = [\n",
      "    \"2.2.0\",\n",
      "    \"2.3.0\",\n",
      "    \"2.4.0\",\n",
      "    \"2.5.0\",\n",
      "    \"2.6.0\",\n",
      "    \"2.7.0\",\n",
      "    \"2.8.0\",\n",
      "    \"3.0.0\",\n",
      "    \"3.1.0\",\n",
      "    \"3.2.0\",\n",
      "    \"4.0.0\", # Calling this 4.0 for now\n",
      "]\n",
      "\n",
      "\n",
      "class TrainableNode(nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        name: str,\n",
      "      ...\n",
      "\n",
      "Chunk 3:\n",
      "Parameters\n",
      "        ----------\n",
      "        name : str\n",
      "            The name of the node.\n",
      "        input_nodes : list of TrainableNode, optional\n",
      "            List of input nodes connected to this node. Defaults to None.\n",
      "        initial_weights : list of float, optional\n",
      "            List of initial weights to  ...\n"
     ]
    }
   ],
   "source": [
    "# Get documents specifically from trainable_heuristic_model.py\n",
    "result = vectorstore.get()\n",
    "relevant_chunks = []\n",
    "\n",
    "for i, metadata in enumerate(result['metadatas']):\n",
    "    if os.path.basename(metadata.get('source', '')) == 'trainable_heuristic_model.py':\n",
    "        relevant_chunks.append({\n",
    "            'content': result['documents'][i],\n",
    "            'metadata': metadata\n",
    "        })\n",
    "\n",
    "print(f\"\\nFound {len(relevant_chunks)} chunks from trainable_heuristic_model.py:\")\n",
    "for i, chunk in enumerate(relevant_chunks[:3], 1):  # Show first 3 chunks\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk['content'][:300], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_llm",
   "language": "python",
   "name": "local_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
